{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Python's `for` loop, we can iterate through a collection of items and take actions on each.\n",
    "The following code creates a list of dictionaries, where each dictionary represents a certain character and contains their page URL and the name of the file where their scraped data should be saved.\n",
    "Loop through it, extract the same information from their page as you did from Harry Potter's, and store it in the appropriate filename.\n",
    "\n",
    "```python\n",
    "characters = [\n",
    "    {'url': 'https://en.wikipedia.org/wiki/Prince_Caspian_(character)', 'filename': 'caspian.txt'},\n",
    "    {'url': 'https://en.wikipedia.org/wiki/Oliver_Twist_(character)', 'filename': 'oliver_twist.txt'},\n",
    "    {'url': 'https://en.wikipedia.org/wiki/Jay_Gatsby', 'filename': 'gatsby.txt'},\n",
    "]\n",
    "```\n",
    "\n",
    "To get you started, your loop might begin like this:\n",
    "\n",
    "```python\n",
    "for character in characters:\n",
    "    url = character['url']\n",
    "    filename = character['filename']\n",
    "    ...\n",
    "```\n",
    "\n",
    "In Jupyter, loops must be entirely defined in a single cell -- so you may need to condense your code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution\n",
    "\n",
    "Again, most of this code can come from our in-class example -- but now we need to put it all inside the body of this `for` loop, so we can execute all of it for each of the characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Always good to start by importing libraries we know we'll need.\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "characters = [\n",
    "    {'url': 'https://en.wikipedia.org/wiki/Prince_Caspian_(character)', 'filename': 'caspian.txt'},\n",
    "    {'url': 'https://en.wikipedia.org/wiki/Oliver_Twist_(character)', 'filename': 'oliver_twist.txt'},\n",
    "    {'url': 'https://en.wikipedia.org/wiki/Jay_Gatsby', 'filename': 'gatsby.txt'},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished scraping https://en.wikipedia.org/wiki/Prince_Caspian_(character). Details written to caspian.txt\n",
      "Finished scraping https://en.wikipedia.org/wiki/Oliver_Twist_(character). Details written to oliver_twist.txt\n",
      "Finished scraping https://en.wikipedia.org/wiki/Jay_Gatsby. Details written to gatsby.txt\n"
     ]
    }
   ],
   "source": [
    "for character in characters:\n",
    "    url = character['url']\n",
    "    filename = character['filename']\n",
    "    # Fetch and parse the HTML\n",
    "    response = requests.get(url)\n",
    "    html = response.content\n",
    "    bs = BeautifulSoup(html, 'html.parser')\n",
    "    # Get the title\n",
    "    title = bs.title.string\n",
    "    # Get the image URL\n",
    "    infobox = bs.find(name='table', class_='infobox')\n",
    "    image = infobox.img\n",
    "    image_url = image['src']\n",
    "    # Get the page text\n",
    "    all_paragraphs = bs.find(id='mw-content-text').find_all('p')\n",
    "    p_text = ''\n",
    "    for paragraph in all_paragraphs: # Notice that loops can be defined within other loops!\n",
    "        p_text = p_text + ''.join(paragraph.strings)\n",
    "        p_text = p_text + '\\n'\n",
    "    # Save the file with our scraped info\n",
    "    with open(filename, 'w') as f:\n",
    "        f.write('Title: ')\n",
    "        f.write(title)\n",
    "        f.write('\\n')\n",
    "        f.write('Image URL: ')\n",
    "        f.write(image_url)\n",
    "        f.write('\\n')\n",
    "        f.write(p_text)\n",
    "    # Print a message so we can see when we've finished each iteration of the loop.\n",
    "    print(f'Finished scraping {url}. Details written to {filename}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Challege\n",
    "If you're familiar with writing functions, try to define a function `scrape_to_file` so that your loop can be simply the following:\n",
    "\n",
    "```python\n",
    "for character in characters:\n",
    "    scrape_to_file(character['url'], character['filename'])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "This is pretty straightforward if we've already done the above -- just move most of that code into a function and make it take arguments for URL and filename."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_to_file(url, filename):\n",
    "    '''\n",
    "    Scrape a character's Wikipedia page and store the details in a given file.\n",
    "    '''\n",
    "    # Fetch and parse the HTML\n",
    "    response = requests.get(url)\n",
    "    html = response.content\n",
    "    bs = BeautifulSoup(html, 'html.parser')\n",
    "    # Get the title\n",
    "    title = bs.title.string\n",
    "    # Get the image URL\n",
    "    infobox = bs.find(name='table', class_='infobox')\n",
    "    image = infobox.img\n",
    "    image_url = image['src']\n",
    "    # Get the page text\n",
    "    all_paragraphs = bs.find(id='mw-content-text').find_all('p')\n",
    "    p_text = ''\n",
    "    for paragraph in all_paragraphs: # Notice that loops can be defined within other loops!\n",
    "        p_text = p_text + ''.join(paragraph.strings)\n",
    "        p_text = p_text + '\\n'\n",
    "    # Save the file with our scraped info\n",
    "    with open(filename, 'w') as f:\n",
    "        f.write('Title: ')\n",
    "        f.write(title)\n",
    "        f.write('\\n')\n",
    "        f.write('Image URL: ')\n",
    "        f.write(image_url)\n",
    "        f.write('\\n')\n",
    "        f.write(p_text)\n",
    "    # Print a message so we can see when we've finished each iteration of the loop.\n",
    "    print(f'Finished scraping {url}. Details written to {filename}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished scraping https://en.wikipedia.org/wiki/Prince_Caspian_(character). Details written to caspian.txt\n",
      "Finished scraping https://en.wikipedia.org/wiki/Oliver_Twist_(character). Details written to oliver_twist.txt\n",
      "Finished scraping https://en.wikipedia.org/wiki/Jay_Gatsby. Details written to gatsby.txt\n"
     ]
    }
   ],
   "source": [
    "for character in characters:\n",
    "    scrape_to_file(character['url'], character['filename'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uc-python-scraping",
   "language": "python",
   "name": "uc-python-scraping"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
